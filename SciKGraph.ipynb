{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import networkx as nx\n",
    "import re\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from pybabelfy.babelfy import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from math import log\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import OClustR as OCR\n",
    "import operator\n",
    "#import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = ''\n",
    "inputFile = ''\n",
    "outputDirectory = ''\n",
    "distance_window = 0\n",
    "graphName = []\n",
    "dictionaries = []\n",
    "dictionariesCode = []\n",
    "graphsI = []\n",
    "graphsD = []\n",
    "sciKGraph = 0\n",
    "pre_processed_graph = 0\n",
    "dictionaryCodeMerged = {}\n",
    "language = ''\n",
    "deleted_nodes = 0\n",
    "deleted_edges = 0\n",
    "deleted_isolated_nodes = 0\n",
    "name=\"\"\n",
    "clusters = []\n",
    "crisp_clusters = []\n",
    "pre_processed_graph = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank Concepts\n",
    "def rank(g, dictionaryCodeMerged):\n",
    "        grau = nx.degree_centrality(g)\n",
    "        sorted_grau = sorted(grau.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        sorted_concepts = []\n",
    "        for i in sorted_grau:\n",
    "            sorted_concepts.append(dictionaryCodeMerged[i[0]].lower().replace('+', ' ') + '  :  ' + i[0])\n",
    "\n",
    "        return sorted_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key Concepts\n",
    "def key_concepts( g, dictionaryCodeMerged):\n",
    "    grau = nx.degree_centrality(g)\n",
    "    sorted_grau = sorted(grau.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_concepts = []\n",
    "    for i in sorted_grau:\n",
    "        sorted_concepts.append([dictionaryCodeMerged[i[0]], i[1]])\n",
    "\n",
    "    return sorted_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and close file\n",
    "def open_file(fileName):\n",
    "    file = open(fileName,\"r\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse and split text in chuncks of at most 3000 characters\n",
    "def parse_text(text):\n",
    "    #remove special characters\n",
    "    punctuationToRemove = string.punctuation.replace('!','').replace('.','').replace('?','').replace('-','').replace(',','')\n",
    "    translator = str.maketrans('', '', punctuationToRemove)\n",
    "    parsedText = text.translate(translator)\n",
    "    #remove numbers\n",
    "    parsedText = re.sub(r'[0-9]+', '', parsedText)\n",
    "    #remove double spaces\n",
    "    parsedText = re.sub(r'  ', ' ', parsedText)\n",
    "    #remove non-printable characters\n",
    "    parsedText = \"\".join(filter(lambda x: x in string.printable, parsedText))\n",
    "    #remove spaces\n",
    "    parsedText = re.sub(r' ', '+', parsedText)\n",
    "    #split text in chuncks of at most 5000 characters\n",
    "    punctuation = ['.','?','!']\n",
    "    splitted_text = []\n",
    "    splitted_text.append(\"\")\n",
    "    n_lines = len(parsedText.splitlines())\n",
    "    for line in parsedText.splitlines():\n",
    "        if n_lines == 1:\n",
    "            splitted_text[-1] = line\n",
    "        else:\n",
    "            if len(splitted_text[-1] + line) < 4500 and splitted_text[-1][-1:] not in punctuation or len(splitted_text[-1] + line) <= 3000:\n",
    "                splitted_text[-1] = splitted_text[-1] + '+' + line\n",
    "            else:\n",
    "                splitted_text.append(line)\n",
    "    translator = str.maketrans('', '', \"?!.\")\n",
    "    for l in splitted_text:\n",
    "        l = l.translate(translator)\n",
    "    return splitted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frag(semantic_annotation, input_text):\n",
    "    start = semantic_annotation.char_fragment_start()\n",
    "    end = semantic_annotation.char_fragment_end()\n",
    "    return input_text[start:end+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def babelfy(lang, key, splitted_text):\n",
    "    babelapi = Babelfy()\n",
    "    paragraphs_annotations = []\n",
    "    paragraphs_text = []\n",
    "    paragraphs_code = []\n",
    "\n",
    "    count = 0\n",
    "    for paragraph in splitted_text: #annotate each paragraph\n",
    "        words_annotations = []\n",
    "        words_text = []\n",
    "        words_code = []\n",
    "        semantic_annotations = babelapi.disambiguate(paragraph,lang,key,match=\"EXACT_MATCHING\",cands=\"TOP\",mcs=\"ON\",anntype=\"ALL\")\n",
    "\n",
    "        #exclude unused annotations (single words of multiword expressions)\n",
    "        for semantic_annotation in semantic_annotations:\n",
    "            if len(words_annotations) == 0 or words_annotations[-1].char_fragment_end() < semantic_annotation.char_fragment_start():\n",
    "                words_annotations.append(semantic_annotation)\n",
    "                words_text.append(frag(semantic_annotation,paragraph))\n",
    "                words_code.append(semantic_annotation.babel_synset_id())\n",
    "\n",
    "            elif words_annotations[-1].char_fragment_start() == semantic_annotation.char_fragment_start():\n",
    "                del words_annotations[-1]\n",
    "                words_annotations.append(semantic_annotation)\n",
    "                del words_text[-1]\n",
    "                words_text.append(frag(semantic_annotation,paragraph))\n",
    "                del words_code[-1]\n",
    "                words_code.append(semantic_annotation.babel_synset_id())\n",
    "\n",
    "\n",
    "        paragraphs_annotations.append(words_annotations)\n",
    "        paragraphs_text.append(words_text)\n",
    "        paragraphs_code.append(words_code)\n",
    "        count = count + 1\n",
    "        print(str(count) + '/' + str(len(splitted_text)))\n",
    "    return paragraphs_annotations, paragraphs_text, paragraphs_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(paragraphs_text, paragraphs_code):\n",
    "    ###        dictionary[word] = code             ###\n",
    "    ###        dictionaryCode[code] = word        ###\n",
    "    ###        weight[code] = weight            ###\n",
    "    dictionary={}\n",
    "    weight={}\n",
    "    dictionaryCode={}\n",
    "    for paragraph, codes in zip(paragraphs_text, paragraphs_code):\n",
    "        for word, code in zip(paragraph, codes):\n",
    "            if code not in weight:\n",
    "                weight[code] = 1\n",
    "            else:\n",
    "                weight[code] = weight[code] + 1\n",
    "\n",
    "            if word not in dictionary:\n",
    "                dictionary[word] = code\n",
    "            if code not in dictionaryCode:\n",
    "                dictionaryCode[code] = word\n",
    "    return dictionary, dictionaryCode, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_graph(peso, paragraphs_code, dictionaryCode, dist):\n",
    "    g = nx.DiGraph() #indirect Graph\n",
    "    g2 = nx.DiGraph() #direct Grap\n",
    "\n",
    "    #calc the weight of each vertice\n",
    "    for code, weight in peso.items():\n",
    "        g.add_node(code, peso=weight, dicionario=dictionaryCode[code])\n",
    "        g2.add_node(code, peso=weight, dicionario=dictionaryCode[code])\n",
    "\n",
    "    #create and weight edges\n",
    "    for line in paragraphs_code:\n",
    "        i = 0\n",
    "        for word in line:\n",
    "            i = i + 1\n",
    "            j = 0\n",
    "            for word2 in line:\n",
    "                j = j + 1\n",
    "                if j - i < dist and j - i > 0: #indirect edges\n",
    "                    if g.has_edge(word, word2):\n",
    "                        g[word][word2]['weight'] += 1 - log(j-i,dist)\n",
    "                    else:\n",
    "                        if word != word2:\n",
    "                            g.add_edge(word, word2, weight=float(1 - log(j-i,dist)))\n",
    "                if j - i == 1: #direct edges\n",
    "                    if g2.has_edge(word, word2):\n",
    "                        g2[word][word2]['weight'] += 1\n",
    "                    else:\n",
    "                        if word != word2:\n",
    "                            g2.add_edge(word, word2, weight=1)\n",
    "    return g, g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clusters_txt(saveFile, Clusters):\n",
    "    f=open(saveFile,\"w+\")\n",
    "    for c in Clusters:\n",
    "        line = ''\n",
    "        for n in c:\n",
    "            line += n + ' '\n",
    "        f.write(line[:-1] + '\\n')\n",
    "    f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveClusters(saveFile=\"\", Clusters=[], crisp=\"\", clusterType='normal'):\n",
    "    file = ''\n",
    "    #save clusters\n",
    "\n",
    "    #write crisp\n",
    "    if crisp != \"\":\n",
    "        with open(saveFile + \"crisp.pickle\", \"wb\") as fp:\n",
    "            pickle.dump(crisp, fp, protocol=2)\n",
    "\n",
    "        f=open(saveFile + \"crisp.txt\",\"w+\")\n",
    "        for c in crisp:\n",
    "            line = ''\n",
    "            for n in c:\n",
    "                line += n + ' '\n",
    "            f.write(line[:-1] + '\\n')\n",
    "        f.close()\n",
    "\n",
    "    #write normal clusters\n",
    "    if clusterType =='normal':\n",
    "        with open(saveFile + \"clusters.pickle\", \"wb\") as fp:\n",
    "            pickle.dump(Clusters, fp, protocol=2)\n",
    "\n",
    "        f=open(saveFile + \"clusters.txt\",\"w+\")\n",
    "        for c in Clusters:\n",
    "            line = ''\n",
    "            for n in c:\n",
    "                line += n + ' '\n",
    "            f.write(line[:-1] + '\\n')\n",
    "        f.close()\n",
    "\n",
    "    #write reduced clusters\n",
    "    elif clusterType =='reduced':\n",
    "        with open(saveFile + \"reducedClusters.pickle\", \"wb\") as fp:\n",
    "            pickle.dump(Clusters, fp, protocol=2)\n",
    "\n",
    "        f=open(saveFile + \"reducedClusters.txt\",\"w+\")\n",
    "        for c in Clusters:\n",
    "            line = ''\n",
    "            for n in c:\n",
    "                line += n + ' '\n",
    "            f.write(line[:-1] + '\\n')\n",
    "        f.close()\n",
    "\n",
    "    else:\n",
    "        print('Wrong cluster Type!\\nCluster not saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_variables_pickle():\n",
    "    save = []\n",
    "    save.append(graphName)\n",
    "    save.append(dictionaries)\n",
    "    save.append(dictionariesCode)\n",
    "    save.append(graphsI)\n",
    "    save.append(graphsD)\n",
    "    save.append(dictionaryCodeMerged)\n",
    "    save.append(sciKGraph)\n",
    "    save.append(crisp_clusters)\n",
    "    save.append(pre_processed_graph)\n",
    "    save.append(clusters)\n",
    "\n",
    "    file = pickle.dumps(save, protocol=2)\n",
    "    #with open('/home/mauro/Downloads/testeDownload.sckg', \"wb\") as fp:\n",
    "    #    pickle.dump(save, fp, protocol=2)\n",
    "\n",
    "\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_variables(output_file, save_graph_name=False, save_directories = False, save_directories_code = False, save_graphs_i = False, save_graphs_d = False, save_directories_code_merged = False, save_SciKGraph = False, save_clusters = False, save_crisp_clusters = False, save_pre_processed_graph = False):\n",
    "    save = []\n",
    "    save.append(graphName)\n",
    "    save.append(dictionaries)\n",
    "    save.append(dictionariesCode)\n",
    "    save.append(graphsI)\n",
    "    save.append(graphsD)\n",
    "    save.append(dictionaryCodeMerged)\n",
    "    save.append(sciKGraph)\n",
    "    save.append(crisp_clusters)\n",
    "    save.append(pre_processed_graph)\n",
    "    save.append(clusters)\n",
    "\n",
    "\n",
    "    try:\n",
    "        with open(output_file, \"wb\") as fp:\n",
    "            pickle.dump(save, fp, protocol=2)\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_variables_pickle(file):\n",
    "    data = pickle.load(file)\n",
    "\n",
    "    graphName = data[0]\n",
    "    dictionaries = data[1]\n",
    "    dictionariesCode = data[2]\n",
    "    graphsI = data[3]\n",
    "    graphsD = data[4]\n",
    "    dictionaryCodeMerged = data[5]\n",
    "    sciKGraph = data[6]\n",
    "    crisp_clusters = data[7]\n",
    "    pre_processed_graph = data[8]\n",
    "    clusters = data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_variables(open_directory, open_graph_name=False, open_directories = False, open_directories_code = False, open_graph_i = False, open_graph_d = False, open_dictionary_code_merged = False, open_SciKGraph = False, open_clusters = False, open_crisp_clusters = False, open_pre_processed_graph = False):\n",
    "\n",
    "    with open(open_directory, \"rb\") as fp:\n",
    "        data = pickle.load(fp)\n",
    "\n",
    "    graphName = data[0]\n",
    "    dictionaries = data[1]\n",
    "    dictionariesCode = data[2]\n",
    "    graphsI = data[3]\n",
    "    graphsD = data[4]\n",
    "    dictionaryCodeMerged = data[5]\n",
    "    sciKGraph = data[6]\n",
    "    crisp_clusters = data[7]\n",
    "    pre_processed_graph = data[8]\n",
    "    clusters = data[9]\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_variables():\n",
    "    key = ''\n",
    "    inputFile = ''\n",
    "    outputDirectory = ''\n",
    "    distance_window = 0\n",
    "    graphName = []\n",
    "    dictionaries = []\n",
    "    dictionariesCode = []\n",
    "    graphsI = []\n",
    "    graphsD = []\n",
    "    sciKGraph = 0\n",
    "    dictionaryCodeMerged = {}\n",
    "    name=\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_SciKGraph(filename, babelfy_key, language, distance_window):\n",
    "    text = filename.decode('ascii')\n",
    "    st = parse_text(text)\n",
    "    pa, pt, pc  = babelfy(language, babelfy_key, st)\n",
    "    d, dc, p = create_dicts(pt, pc)\n",
    "\n",
    "    gI, gD = create_simple_graph(p, pc, dc, distance_window)\n",
    "    return d, dc, gI, gD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merges graphs and dictionaries\n",
    "## graphs: list of graphs to merge\n",
    "## dictionaryCode: list of the graphs dictionaries\n",
    "def merge_graphs(graphs, dictionaryCode):\n",
    "    #create dictionaryCodeMerged\n",
    "    dictionaryCodeMerged = {}\n",
    "    for dic in dictionaryCode:\n",
    "        for w in dic:\n",
    "            if w not in dictionaryCodeMerged:\n",
    "                dictionaryCodeMerged[w] = dic[w]\n",
    "\n",
    "\n",
    "    #merge graphs\n",
    "    graph = nx.compose_all(graphs).copy()\n",
    "\n",
    "    #reset nodes weights\n",
    "    for i in graph.nodes():\n",
    "        graph.nodes()[i]['peso'] = 0\n",
    "\n",
    "    #recalc nodes weights\n",
    "    for i in range(len(graphs)):\n",
    "        for n in graphs[i]:\n",
    "            graph.nodes()[n]['peso'] += graphs[i].nodes()[n]['peso']\n",
    "            graph.nodes()[n]['dicionario'] = dictionaryCodeMerged[n]\n",
    "\n",
    "    #reset arc weight\n",
    "    for i in graph.edges():\n",
    "        graph[i[0]][i[1]]['weight'] = 0\n",
    "\n",
    "    #recalc arc weight\n",
    "    for i in range(len(graphs)):\n",
    "        for e in graphs[i].edges():\n",
    "            graph[e[0]][e[1]]['weight'] += graphs[i][e[0]][e[1]]['weight']\n",
    "\n",
    "\n",
    "\n",
    "    return graph, dictionaryCodeMerged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SciKGraph(files, file_names, babelfy_key = None, language = 'EN', graphType = 'direct', distance_window=2, mergeIfFail = False):\n",
    "    distance_window = distance_window + 1\n",
    "    if distance_window <=2:\n",
    "        graphType = 'direct'\n",
    "    else:\n",
    "        graphType = 'indirect'\n",
    "\n",
    "    language = language\n",
    "\n",
    "    #check if scikgraph should be fully updated (occurs when distance window changes)\n",
    "    if distance_window != distance_window:\n",
    "        distance_window = distance_window\n",
    "        graphName = []\n",
    "\n",
    "    toMerge = []\n",
    "    count = 0\n",
    "    added = 0\n",
    "    for file, file_name in zip(files, file_names):\n",
    "        count += 1\n",
    "        if file_name not in graphName:\n",
    "            try:\n",
    "                d, dc, gI, gD = create_single_SciKGraph(file, babelfy_key, language, distance_window)\n",
    "\n",
    "                graphName.append(file_name)\n",
    "                dictionaries.append(d)\n",
    "                dictionariesCode.append(dc)\n",
    "                graphsI.append(gI)\n",
    "                graphsD.append(gD)\n",
    "                added += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                if len(graphName) > 0 or mergeIfFail:\n",
    "                    print('Error Babelfying text (check your Babelcoins)\\n', e, '\\n')\n",
    "                    print(graphName, '\\nThe documents in \\'graphName\\' were correctly babelfied.\\nThe SciKGraph was created with the correctly babelfied texts, to update this version with the other texts fix the error (probably babelfy key error) and run this method again.')\n",
    "                    break\n",
    "                else:\n",
    "                    if len(graphName) > 0:\n",
    "                        print(graphName, '\\nThe documents in \\'graphName\\' were correctly babelfied.\\nTo create the SciKGraph (using the previously babelfied documents) run this method again.\\n')\n",
    "                    print('Error Babelfying text (check your Babelcoins)\\n')\n",
    "                    raise\n",
    "\n",
    "        if graphType == 'direct':\n",
    "            toMerge = graphsD\n",
    "        elif graphType == 'indirect':\n",
    "            toMerge = graphsI\n",
    "        else:\n",
    "            print('graphType not listed!\\nDirect graph used.')\n",
    "            toMerge = graphsD\n",
    "\n",
    "    #check if at leat 1 graph can be added to scikgraph\n",
    "    if added > 0:\n",
    "        graph, dictionaryMerged = merge_graphs(toMerge, dictionariesCode)\n",
    "        sciKGraph = graph\n",
    "        dictionaryCodeMerged = dictionaryMerged\n",
    "\n",
    "    return sciKGraph, dictionaryCodeMerged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_communities(g, edges_threshold, nodes_threshold):\n",
    "    ocr = OCR.OClustR()\n",
    "    clusters, crisp_clusters, pre_processed_graph = ocr.identify_clusters(g, edges_threshold, nodes_threshold)\n",
    "    return clusters, crisp_clusters, pre_processed_graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_graph(g):\n",
    "    ocr = OCR.OClustR()\n",
    "    clusters, crisp_clusters, sciKGraph = ocr.cluster_graph(g)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_graph(g, edges_threshold, nodes_threshold, list_edges = [], list_nodes = []):\n",
    "    oClustR = OCR.OClustR()\n",
    "    g, rem_e, rem_n, rem_iso_n = oClustR.pre_process(g, edges_threshold, nodes_threshold, list_edges, list_nodes)\n",
    "    pre_processed_graph = g\n",
    "    deleted_isolated_nodes = rem_iso_n\n",
    "    deleted_nodes = rem_n\n",
    "    deleted_edges = rem_e\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_crisp(Clusters):\n",
    "    ##Crisp Cluster\n",
    "    crisp = []\n",
    "    elem = []\n",
    "    for c in Clusters:\n",
    "        cl = []\n",
    "        for v in c:\n",
    "            if v not in elem:\n",
    "                cl.append(v)\n",
    "                elem.append(v)\n",
    "        if len(cl) >= 1:\n",
    "            crisp.append(cl)\n",
    "    return crisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start( inputDirectory, babelfy_key, edges_threshold=0, nodes_threshold=0, list_nodes = [], list_edges = [], language = 'EN', graphType = 'direct', distance_window=2, mergeIfFail = False):\n",
    "    if babelfy_key == None:\n",
    "        babelfy_key = key\n",
    "\n",
    "    filenames = []\n",
    "\n",
    "    try:\n",
    "        for filename in sorted(glob.glob(os.path.join(inputDirectory, '*.txt'))):\n",
    "            filenames.append(filename)\n",
    "        if len(filename) == 0:\n",
    "            raise EmptyDirectoryError('There is no .txt file in the inputDirectory.')\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "\n",
    "    sciKGraph, dictionaryCodeMerged = create_SciKGraph(filenames, babelfy_key, language, graphType, distance_window, mergeIfFail)\n",
    "    return sciKGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Create SciKGraph #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documentsList (list of files)= list of documents\n",
    "#documentsNamesList (list of strings)= list of the names of the documents in documentsList\n",
    "#babelfy_key (string)= babelfy key \n",
    "#language (string)= 'EN'\n",
    "#distance_window (int) = value of distance to consider of concepts coocorrence\n",
    "#mergeIfFail (boolean) = If Babelfy fail (key problem) merged already babelfied texts?\n",
    "\n",
    "create_SciKGraph(documentsList, documentsNamesList, babelfy_key, language, distance_window, mergeIfFail)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
